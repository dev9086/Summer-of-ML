# ðŸ§  Adult Census Income Classification

This project is focused on predicting whether a person earns more than \$50K a year using the [Adult Census Income Dataset](https://archive.ics.uci.edu/ml/datasets/adult). It involves complete data preprocessing, model evaluation, cross-validation, hyperparameter tuning, and performance comparison across several ML models.

---

## ðŸ“‚ Dataset Overview

- **Source**: UCI Machine Learning Repository
- **Target**: `income` (binary classification â€” `>50K` vs `<=50K`)

---
## ðŸ§¹ Data Preprocessing 

To prepare the data for robust modeling, implemented a structured and repeatable preprocessing pipeline with the following steps:

### âœ… 1. Data Cleaning
- Replaced invalid placeholders (`'?'`, `99999`) with `NaN`.
- Dropped redundant columns like `fnlwgt`.
- Removed exact duplicate rows to reduce data noise.

### âœ… 2. Missing Value Imputation
- **Numerical Features**: Imputed using **mean strategy**.
- **Categorical Features**: Imputed using **most frequent category**.

### âœ… 3. Label Encoding
- Applied **Label Encoding** to all categorical variables 


### âœ… 4. Feature Scaling
- Applied **StandardScaler** to normalize numerical features:
  -`age`, `education-num`, `capital-gain`, `capital-loss`, `hours-per-week`
- This ensures features are on the same scale and models like KNN & Gradient Boosting perform optimally.


## ðŸ“ˆ Models Used

Each model was:
- Evaluated using **Stratified Cross-Validation**
- Tuned with **GridSearchCV**
- Assessed using **Accuracy**, **F1 Score**, and **ROC AUC**

| Model                      | Accuracy | F1 Score | ROC AUC |
|----------------------------|----------|----------|---------|
| âœ… LGBMClassifier           | 0.8716   | 0.7113   | 0.9263  |
| âœ… XGBClassifier            | 0.8708   | 0.7094   | 0.9263  |
| âœ… GradientBoosting        | 0.8708   | 0.7098   | 0.9259  |
| âœ… RandomForestClassifier   | 0.8615   | 0.6807   | 0.9132  |
| âœ… AdaBoostClassifier       | 0.8612   | 0.6877   | 0.9162  |
| âœ… DecisionTreeClassifier   | 0.8519   | 0.6759   | 0.8972  |
| âœ… KNeighborsClassifier     | 0.8354   | 0.6320   | 0.8821  |

---

## ðŸ† Conclusion

- **LightGBM** was the best performer across all metrics.
- Boosting models clearly outperformed simpler models.
- KNN had the weakest performance and longest training time.

---

## ðŸ“¦ Libraries Used

- `pandas`, `numpy`, `matplotlib`, `seaborn`
- `scikit-learn`
- `xgboost`, `lightgbm`, `catboost`

---
